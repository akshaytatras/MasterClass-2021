{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "11_NLP_Applications_Text_Classification_Machine_Learning_and_Basic_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLOatR1LRaE_"
      },
      "source": [
        "# Sentiment Analysis - Machine Learning and Basic Deep Neural Network Models\n",
        "\n",
        "We have already discussed that sentiment analysis, also popularly known as opinion analysis or opinion mining is one of the most important applications of NLP. The key idea is to predict the potential sentiment of a body of text based on the textual content. In this sub-unit, we will be exploring supervised learning models. \n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_cover.png?raw=1)\n",
        "\n",
        "Another way to build a model to understand the text content and predict the sentiment of the text based reviews is to use supervised machine learning. To be more specific, we will be using classification models for solving this problem. We will be building an automated sentiment text classification system in subsequent sections. The major steps to achieve this are mentioned as follows.\n",
        "\n",
        "1.\tPrepare train and test datasets (optionally a validation dataset)\n",
        "2.\tPre-process and normalize text documents\n",
        "3.\tFeature Engineering \n",
        "4.\tModel training\n",
        "5.\tModel prediction and evaluation\n",
        "\n",
        "These are the major steps for building our system. Optionally the last step would be to deploy the model in your server or on the cloud. The following figure shows a detailed workflow for building a standard text classification system with supervised learning (classification) models.\n",
        "\n",
        "![](https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/sentiment_classifier_workflow.png?raw=1)\n",
        "\n",
        "\n",
        "In our scenario, documents indicate the movie reviews and classes indicate the review sentiments which can either be positive or negative making it a binary classification problem. We will build models using both traditional machine learning methods and newer deep learning in the subsequent sections. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "610bNp_4SQma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862da810-1942-4111-942d-d7ff5be482ac"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/04/d5e0bb9f2cef5d15616ebf68087a725c5dbdd71bd422bcfb35d709f98ce7/contractions-0.0.48-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/c7/61370d9e3c349478e89a5554c1e5d9658e1e3116cc4f2528f568909ebdf1/anyascii-0.1.7-py3-none-any.whl (260kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 8.6MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 15.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85389 sha256=8c4351163d40c754c59923d1cff323f9f0312a87164fbe7a92e79c093a88af71\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: anyascii, pyahocorasick, textsearch, contractions\n",
            "Successfully installed anyascii-0.1.7 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.7/dist-packages (0.0.21)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch) (1.4.2)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch) (0.1.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMFvLtEHRaFM"
      },
      "source": [
        "# Load and View Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgphrYufRaFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2bbfe09-2297-40b9-eee1-a6c347c1f7a2"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2')\n",
        "dataset.info()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nkEEGExRaFc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f603690c-146e-4b60-8da4-b46b884206d3"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V1NWGhcRaFi"
      },
      "source": [
        "# Build Train and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JP10IEYRaFj"
      },
      "source": [
        "# build train and test datasets\n",
        "reviews = dataset['review'].values\n",
        "sentiments = dataset['sentiment'].values\n",
        "\n",
        "train_reviews = reviews[:35000]\n",
        "train_sentiments = sentiments[:35000]\n",
        "\n",
        "test_reviews = reviews[35000:]\n",
        "test_sentiments = sentiments[35000:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i45AFxXNRaFn"
      },
      "source": [
        "# Text Wrangling & Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHZ0lEGNRaFo"
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm.tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO3ESug2RaFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0be1464-adcb-4f75-8d55-1ad46ae72b7d"
      },
      "source": [
        "%%time\n",
        "\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 35000/35000 [00:16<00:00, 2176.89it/s]\n",
            "100%|██████████| 15000/15000 [00:06<00:00, 2160.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.8 s, sys: 153 ms, total: 23 s\n",
            "Wall time: 23 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucDv8n50RaFu"
      },
      "source": [
        "# Traditional Supervised Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOZ7Rn0jRaFv"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD8q5QoERaFw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090206b6-2865-4ff0-f1ed-20db8cac768a"
      },
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
        "\n",
        "\n",
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 41.5 s, sys: 878 ms, total: 42.3 s\n",
            "Wall time: 42.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSvqpHRYRaFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467b847c-626c-4171-b4a2-f3453671ceac"
      },
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.6 s, sys: 31.7 ms, total: 10.6 s\n",
            "Wall time: 10.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfQPYw8PRaF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99cd88f-3039-4975-8d7f-6410f0419559"
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (35000, 194907)  Test features shape: (15000, 194907)\n",
            "TFIDF model:> Train features shape: (35000, 194907)  Test features shape: (15000, 194907)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aUUsxMrRaF7"
      },
      "source": [
        "## Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPA5UtYFRaF8"
      },
      "source": [
        "### Try out Logistic Regression\n",
        "\n",
        "The logistic regression model is actually a statistical model developed by statistician\n",
        "David Cox in 1958. It is also known as the logit or logistic model since it uses the\n",
        "logistic (popularly also known as sigmoid) mathematical function to estimate the\n",
        "parameter values. These are the coefficients of all our features such that the overall loss\n",
        "is minimized when predicting the outcome—"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGHQErnMRaF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25fceda-4067-488b-b373-e40aef6bc09e"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate model\n",
        "lr = LogisticRegression(penalty='l2', max_iter=700, C=1, solver='lbfgs', random_state=42)\n",
        "\n",
        "# train model\n",
        "lr.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_bow_predictions = lr.predict(cv_test_features)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 21s, sys: 1min 27s, total: 2min 49s\n",
            "Wall time: 1min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqeRyayrRaGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "86975575-e2f1-46eb-ba1b-d9751be7a13b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.90      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6751</td>\n",
              "      <td>739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>713</td>\n",
              "      <td>6797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6751       739\n",
              "positive       713      6797"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQEAz6O6RaGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4636f72-b898-442f-ce2b-7fc61d54f48c"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Logistic Regression model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "lr.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "lr_tfidf_predictions = lr.predict(tv_test_features)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.25 s, sys: 3.29 s, total: 6.54 s\n",
            "Wall time: 3.44 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNCfZnUORaGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "8d763dc4-4ef7-44dc-9275-39e042f64fea"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, lr_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.89      0.90      7490\n",
            "    positive       0.90      0.91      0.90      7510\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6691</td>\n",
              "      <td>799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>670</td>\n",
              "      <td>6840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6691       799\n",
              "positive       670      6840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoOoEAiXRaGH"
      },
      "source": [
        "### Try out Random Forest\n",
        "\n",
        "Decision trees are a family of supervised machine learning algorithms that can represent\n",
        "and interpret sets of rules automatically from the underlying data. They use metrics like\n",
        "information gain and gini-index to build the tree. However, a major drawback of decision\n",
        "trees is that since they are non-parametric, the more data there is, greater the depth of\n",
        "the tree. We can end up with really huge and deep trees that are prone to overfitting. The\n",
        "model might work really well on training data, but instead of learning, it just memorizes\n",
        "all the training samples and builds very specific rules to them. Hence, it performs really\n",
        "poorly on the test data. Random forests try to tackle this problem.\n",
        "\n",
        "A random forest is a meta-estimator or an ensemble model that fits a number of\n",
        "decision tree classifiers on various sub-samples of the dataset and uses averaging to\n",
        "improve the predictive accuracy and control over-fitting. The sub-sample size is always\n",
        "the same as the original input sample size, but the samples are drawn with replacement\n",
        "(bootstrap samples). In random forests, all the trees are trained in parallel (bagging\n",
        "model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a\n",
        "sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also,\n",
        "when splitting a node during the construction of the tree, the split that is chosen is no\n",
        "longer the best split among all features. Instead, the split that is picked is the best split\n",
        "among a random subset of the features. T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzaSSOpYRaGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9348c0a-7ff2-43f7-8526-be521da09370"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on BOW features\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate model\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "\n",
        "# train model\n",
        "rf.fit(cv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_bow_predictions = rf.predict(cv_test_features)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 22s, sys: 164 ms, total: 3min 22s\n",
            "Wall time: 1min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "617Kuv7_RaGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "256e3aaa-febc-4772-d5a2-bd42c6c64ddd"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_bow_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_bow_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.86      0.86      7490\n",
            "    positive       0.86      0.86      0.86      7510\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6411</td>\n",
              "      <td>1079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1079</td>\n",
              "      <td>6431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6411      1079\n",
              "positive      1079      6431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdvmBOrPRaGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217984c-ea20-48dc-927c-0652836cb1d7"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Random Forest model on TF-IDF features\n",
        "\n",
        "# train model\n",
        "rf.fit(tv_train_features, train_sentiments)\n",
        "\n",
        "# predict on test data\n",
        "rf_tfidf_predictions = rf.predict(tv_test_features)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 55s, sys: 222 ms, total: 2min 56s\n",
            "Wall time: 1min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hBOMh6uRaGP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "43ac0c3c-c6ad-409c-979c-212199e3e378"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, rf_tfidf_predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, rf_tfidf_predictions), index=labels, columns=labels)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.86      0.85      7490\n",
            "    positive       0.86      0.84      0.85      7510\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6458</td>\n",
              "      <td>1032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>1212</td>\n",
              "      <td>6298</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6458      1032\n",
              "positive      1212      6298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoqZhMQFRaGS"
      },
      "source": [
        "# Newer Supervised Deep Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZw6LYNHRaGT"
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiZbcv_gRaGZ"
      },
      "source": [
        "## Prediction class label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnhC4rWaRaGb"
      },
      "source": [
        "le = LabelEncoder()\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in norm_train_reviews]\n",
        "y_train = le.fit_transform(train_sentiments)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                       for text in norm_test_reviews]\n",
        "y_test = le.fit_transform(test_sentiments)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ogDRDh4RaGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b4fb8a-bafe-4ac4-a4b9-0fbdb40e7f53"
      },
      "source": [
        "# print class label encoding map and encoded labels\n",
        "print('Sentiment class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
        "print('Sample test label transformation:\\n'+'-'*35,\n",
        "      '\\nActual Labels:', test_sentiments[:3], '\\nEncoded Labels:', y_test[:3])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment class label map: {'negative': 0, 'positive': 1}\n",
            "Sample test label transformation:\n",
            "----------------------------------- \n",
            "Actual Labels: ['negative' 'positive' 'negative'] \n",
            "Encoded Labels: [0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdexbrYXRaGk"
      },
      "source": [
        "## Feature Engineering with word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5S0u0BbiN2a"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9kfCw6LRaGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb11097a-677d-44ae-96b5-aad86a5957e6"
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 300\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)    "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 04:38:30,369 : INFO : collecting all words and their counts\n",
            "2021-04-08 04:38:30,370 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-04-08 04:38:30,772 : INFO : PROGRESS: at sentence #10000, processed 2295247 words, keeping 82421 word types\n",
            "2021-04-08 04:38:31,197 : INFO : PROGRESS: at sentence #20000, processed 4591737 words, keeping 124842 word types\n",
            "2021-04-08 04:38:31,610 : INFO : PROGRESS: at sentence #30000, processed 6885438 words, keeping 159845 word types\n",
            "2021-04-08 04:38:31,816 : INFO : collected 176281 word types from a corpus of 8036512 raw words and 35000 sentences\n",
            "2021-04-08 04:38:31,818 : INFO : Loading a fresh vocabulary\n",
            "2021-04-08 04:38:31,928 : INFO : effective_min_count=10 retains 24650 unique words (13% of original 176281, drops 151631)\n",
            "2021-04-08 04:38:31,930 : INFO : effective_min_count=10 leaves 7764210 word corpus (96% of original 8036512, drops 272302)\n",
            "2021-04-08 04:38:32,002 : INFO : deleting the raw counts dictionary of 176281 items\n",
            "2021-04-08 04:38:32,006 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2021-04-08 04:38:32,007 : INFO : downsampling leaves estimated 5720836 word corpus (73.7% of prior 7764210)\n",
            "2021-04-08 04:38:32,070 : INFO : estimated required memory for 24650 words and 300 dimensions: 71485000 bytes\n",
            "2021-04-08 04:38:32,071 : INFO : resetting layer weights\n",
            "2021-04-08 04:38:36,453 : INFO : training model with 4 workers on 24650 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2021-04-08 04:38:37,567 : INFO : EPOCH 1 - PROGRESS: at 1.55% examples, 82427 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:38,738 : INFO : EPOCH 1 - PROGRESS: at 3.63% examples, 89368 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:39,815 : INFO : EPOCH 1 - PROGRESS: at 5.62% examples, 93934 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:40,885 : INFO : EPOCH 1 - PROGRESS: at 7.33% examples, 93331 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:38:42,122 : INFO : EPOCH 1 - PROGRESS: at 9.35% examples, 93878 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:43,220 : INFO : EPOCH 1 - PROGRESS: at 11.29% examples, 95155 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:44,363 : INFO : EPOCH 1 - PROGRESS: at 13.27% examples, 95511 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:38:45,491 : INFO : EPOCH 1 - PROGRESS: at 15.12% examples, 95924 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:46,640 : INFO : EPOCH 1 - PROGRESS: at 16.91% examples, 96105 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:47,712 : INFO : EPOCH 1 - PROGRESS: at 18.94% examples, 96895 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:48,724 : INFO : EPOCH 1 - PROGRESS: at 20.75% examples, 97491 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:49,760 : INFO : EPOCH 1 - PROGRESS: at 22.64% examples, 97785 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:50,798 : INFO : EPOCH 1 - PROGRESS: at 24.27% examples, 97062 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:51,904 : INFO : EPOCH 1 - PROGRESS: at 26.27% examples, 97727 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:52,963 : INFO : EPOCH 1 - PROGRESS: at 28.33% examples, 98268 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:53,968 : INFO : EPOCH 1 - PROGRESS: at 30.06% examples, 98554 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:54,995 : INFO : EPOCH 1 - PROGRESS: at 31.84% examples, 98677 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:55,996 : INFO : EPOCH 1 - PROGRESS: at 33.52% examples, 98626 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:57,023 : INFO : EPOCH 1 - PROGRESS: at 35.43% examples, 98828 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:58,216 : INFO : EPOCH 1 - PROGRESS: at 37.39% examples, 98533 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:38:59,330 : INFO : EPOCH 1 - PROGRESS: at 39.31% examples, 98635 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:00,411 : INFO : EPOCH 1 - PROGRESS: at 41.27% examples, 98846 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:01,442 : INFO : EPOCH 1 - PROGRESS: at 43.28% examples, 99258 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:02,528 : INFO : EPOCH 1 - PROGRESS: at 45.00% examples, 98872 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:03,557 : INFO : EPOCH 1 - PROGRESS: at 46.81% examples, 99032 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:04,597 : INFO : EPOCH 1 - PROGRESS: at 48.68% examples, 99111 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:05,675 : INFO : EPOCH 1 - PROGRESS: at 50.71% examples, 99296 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:06,703 : INFO : EPOCH 1 - PROGRESS: at 52.43% examples, 99171 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:07,770 : INFO : EPOCH 1 - PROGRESS: at 54.30% examples, 99151 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:08,873 : INFO : EPOCH 1 - PROGRESS: at 56.26% examples, 99224 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:39:09,953 : INFO : EPOCH 1 - PROGRESS: at 58.17% examples, 99348 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:10,985 : INFO : EPOCH 1 - PROGRESS: at 59.94% examples, 99430 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:12,049 : INFO : EPOCH 1 - PROGRESS: at 61.79% examples, 99409 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:13,063 : INFO : EPOCH 1 - PROGRESS: at 63.62% examples, 99517 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:14,179 : INFO : EPOCH 1 - PROGRESS: at 65.51% examples, 99533 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:15,246 : INFO : EPOCH 1 - PROGRESS: at 67.50% examples, 99700 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:16,341 : INFO : EPOCH 1 - PROGRESS: at 69.48% examples, 99761 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:17,372 : INFO : EPOCH 1 - PROGRESS: at 71.33% examples, 99833 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:18,390 : INFO : EPOCH 1 - PROGRESS: at 73.15% examples, 99901 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:19,483 : INFO : EPOCH 1 - PROGRESS: at 75.12% examples, 99944 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:20,606 : INFO : EPOCH 1 - PROGRESS: at 77.14% examples, 99929 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:21,698 : INFO : EPOCH 1 - PROGRESS: at 79.10% examples, 100004 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:22,739 : INFO : EPOCH 1 - PROGRESS: at 80.90% examples, 100022 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:23,768 : INFO : EPOCH 1 - PROGRESS: at 82.48% examples, 99768 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:24,830 : INFO : EPOCH 1 - PROGRESS: at 84.44% examples, 99750 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:25,892 : INFO : EPOCH 1 - PROGRESS: at 86.22% examples, 99750 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:26,918 : INFO : EPOCH 1 - PROGRESS: at 87.75% examples, 99532 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:27,928 : INFO : EPOCH 1 - PROGRESS: at 89.61% examples, 99631 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:28,973 : INFO : EPOCH 1 - PROGRESS: at 91.12% examples, 99378 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:30,181 : INFO : EPOCH 1 - PROGRESS: at 93.08% examples, 99220 words/s, in_qsize 8, out_qsize 2\n",
            "2021-04-08 04:39:31,306 : INFO : EPOCH 1 - PROGRESS: at 95.12% examples, 99233 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:32,428 : INFO : EPOCH 1 - PROGRESS: at 97.17% examples, 99243 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:33,524 : INFO : EPOCH 1 - PROGRESS: at 98.98% examples, 99290 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:33,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-08 04:39:33,963 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 04:39:33,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 04:39:33,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 04:39:33,996 : INFO : EPOCH - 1 : training on 8036512 raw words (5720238 effective words) took 57.5s, 99422 effective words/s\n",
            "2021-04-08 04:39:35,064 : INFO : EPOCH 2 - PROGRESS: at 1.55% examples, 85991 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:36,177 : INFO : EPOCH 2 - PROGRESS: at 3.63% examples, 93717 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:37,390 : INFO : EPOCH 2 - PROGRESS: at 5.63% examples, 93197 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-08 04:39:38,487 : INFO : EPOCH 2 - PROGRESS: at 7.58% examples, 95259 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:39,506 : INFO : EPOCH 2 - PROGRESS: at 9.32% examples, 96585 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:40,604 : INFO : EPOCH 2 - PROGRESS: at 11.16% examples, 96394 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:41,722 : INFO : EPOCH 2 - PROGRESS: at 13.14% examples, 96911 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:39:42,737 : INFO : EPOCH 2 - PROGRESS: at 14.90% examples, 97667 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:43,911 : INFO : EPOCH 2 - PROGRESS: at 16.69% examples, 97357 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:45,044 : INFO : EPOCH 2 - PROGRESS: at 18.69% examples, 97482 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:46,167 : INFO : EPOCH 2 - PROGRESS: at 20.65% examples, 97695 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:47,237 : INFO : EPOCH 2 - PROGRESS: at 22.64% examples, 98249 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:48,248 : INFO : EPOCH 2 - PROGRESS: at 24.37% examples, 98182 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:49,328 : INFO : EPOCH 2 - PROGRESS: at 26.01% examples, 97565 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:50,350 : INFO : EPOCH 2 - PROGRESS: at 27.95% examples, 97908 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:51,403 : INFO : EPOCH 2 - PROGRESS: at 29.48% examples, 97143 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:52,495 : INFO : EPOCH 2 - PROGRESS: at 31.39% examples, 97417 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:53,541 : INFO : EPOCH 2 - PROGRESS: at 32.92% examples, 96825 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:54,615 : INFO : EPOCH 2 - PROGRESS: at 34.80% examples, 96866 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:55,636 : INFO : EPOCH 2 - PROGRESS: at 36.42% examples, 96477 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:39:56,660 : INFO : EPOCH 2 - PROGRESS: at 38.10% examples, 96445 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:39:57,767 : INFO : EPOCH 2 - PROGRESS: at 39.80% examples, 96081 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:58,835 : INFO : EPOCH 2 - PROGRESS: at 41.69% examples, 96164 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:39:59,952 : INFO : EPOCH 2 - PROGRESS: at 43.40% examples, 95807 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:01,094 : INFO : EPOCH 2 - PROGRESS: at 45.36% examples, 95903 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-08 04:40:02,248 : INFO : EPOCH 2 - PROGRESS: at 47.35% examples, 95981 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:03,273 : INFO : EPOCH 2 - PROGRESS: at 49.29% examples, 96444 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:04,288 : INFO : EPOCH 2 - PROGRESS: at 51.10% examples, 96458 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:05,353 : INFO : EPOCH 2 - PROGRESS: at 52.81% examples, 96326 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:06,571 : INFO : EPOCH 2 - PROGRESS: at 54.84% examples, 96172 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:40:07,580 : INFO : EPOCH 2 - PROGRESS: at 56.57% examples, 96406 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:08,669 : INFO : EPOCH 2 - PROGRESS: at 58.17% examples, 95992 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:09,705 : INFO : EPOCH 2 - PROGRESS: at 59.94% examples, 96152 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:10,711 : INFO : EPOCH 2 - PROGRESS: at 61.55% examples, 96001 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:11,860 : INFO : EPOCH 2 - PROGRESS: at 63.51% examples, 96036 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:12,870 : INFO : EPOCH 2 - PROGRESS: at 65.05% examples, 95881 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:13,989 : INFO : EPOCH 2 - PROGRESS: at 66.85% examples, 95828 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:40:15,005 : INFO : EPOCH 2 - PROGRESS: at 68.77% examples, 96026 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:16,047 : INFO : EPOCH 2 - PROGRESS: at 70.33% examples, 95809 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:17,063 : INFO : EPOCH 2 - PROGRESS: at 72.06% examples, 95822 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:18,153 : INFO : EPOCH 2 - PROGRESS: at 73.71% examples, 95686 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:19,188 : INFO : EPOCH 2 - PROGRESS: at 75.64% examples, 95785 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:20,188 : INFO : EPOCH 2 - PROGRESS: at 77.40% examples, 95836 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:21,286 : INFO : EPOCH 2 - PROGRESS: at 79.10% examples, 95686 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:22,309 : INFO : EPOCH 2 - PROGRESS: at 80.64% examples, 95549 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:23,343 : INFO : EPOCH 2 - PROGRESS: at 82.38% examples, 95523 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:24,529 : INFO : EPOCH 2 - PROGRESS: at 84.44% examples, 95508 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:40:25,535 : INFO : EPOCH 2 - PROGRESS: at 86.22% examples, 95698 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:26,572 : INFO : EPOCH 2 - PROGRESS: at 87.87% examples, 95677 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:27,640 : INFO : EPOCH 2 - PROGRESS: at 89.49% examples, 95474 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:28,663 : INFO : EPOCH 2 - PROGRESS: at 91.12% examples, 95479 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:29,686 : INFO : EPOCH 2 - PROGRESS: at 92.70% examples, 95352 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:40:30,818 : INFO : EPOCH 2 - PROGRESS: at 94.75% examples, 95429 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:40:31,957 : INFO : EPOCH 2 - PROGRESS: at 96.79% examples, 95489 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:32,991 : INFO : EPOCH 2 - PROGRESS: at 98.57% examples, 95606 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:33,579 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-08 04:40:33,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 04:40:33,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 04:40:33,729 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 04:40:33,730 : INFO : EPOCH - 2 : training on 8036512 raw words (5720640 effective words) took 59.7s, 95779 effective words/s\n",
            "2021-04-08 04:40:34,810 : INFO : EPOCH 3 - PROGRESS: at 1.55% examples, 85218 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:35,859 : INFO : EPOCH 3 - PROGRESS: at 3.49% examples, 92801 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:36,940 : INFO : EPOCH 3 - PROGRESS: at 5.37% examples, 94013 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:38,063 : INFO : EPOCH 3 - PROGRESS: at 7.33% examples, 95526 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:39,185 : INFO : EPOCH 3 - PROGRESS: at 9.20% examples, 96287 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:40:40,345 : INFO : EPOCH 3 - PROGRESS: at 11.16% examples, 96349 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:41,476 : INFO : EPOCH 3 - PROGRESS: at 13.14% examples, 96680 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:42,478 : INFO : EPOCH 3 - PROGRESS: at 14.90% examples, 97590 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:43,523 : INFO : EPOCH 3 - PROGRESS: at 16.35% examples, 96367 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:44,644 : INFO : EPOCH 3 - PROGRESS: at 18.30% examples, 96763 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:45,758 : INFO : EPOCH 3 - PROGRESS: at 20.29% examples, 97105 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:46,863 : INFO : EPOCH 3 - PROGRESS: at 22.30% examples, 97498 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:47,976 : INFO : EPOCH 3 - PROGRESS: at 24.27% examples, 97716 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:49,081 : INFO : EPOCH 3 - PROGRESS: at 26.11% examples, 97895 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:50,138 : INFO : EPOCH 3 - PROGRESS: at 28.21% examples, 98481 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:51,185 : INFO : EPOCH 3 - PROGRESS: at 29.95% examples, 98496 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:52,203 : INFO : EPOCH 3 - PROGRESS: at 31.74% examples, 98684 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:53,282 : INFO : EPOCH 3 - PROGRESS: at 33.52% examples, 98585 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:54,319 : INFO : EPOCH 3 - PROGRESS: at 35.46% examples, 98731 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:55,325 : INFO : EPOCH 3 - PROGRESS: at 37.15% examples, 98672 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-08 04:40:56,440 : INFO : EPOCH 3 - PROGRESS: at 39.07% examples, 98729 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:57,483 : INFO : EPOCH 3 - PROGRESS: at 40.90% examples, 98806 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:58,491 : INFO : EPOCH 3 - PROGRESS: at 42.80% examples, 99036 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:40:59,555 : INFO : EPOCH 3 - PROGRESS: at 44.47% examples, 98731 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:00,662 : INFO : EPOCH 3 - PROGRESS: at 46.47% examples, 98860 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:01,699 : INFO : EPOCH 3 - PROGRESS: at 48.44% examples, 99216 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:02,877 : INFO : EPOCH 3 - PROGRESS: at 50.32% examples, 98810 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-08 04:41:03,955 : INFO : EPOCH 3 - PROGRESS: at 52.31% examples, 99015 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:04,994 : INFO : EPOCH 3 - PROGRESS: at 54.16% examples, 99091 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:06,148 : INFO : EPOCH 3 - PROGRESS: at 56.26% examples, 99231 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:07,230 : INFO : EPOCH 3 - PROGRESS: at 58.17% examples, 99354 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:08,261 : INFO : EPOCH 3 - PROGRESS: at 59.83% examples, 99232 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:09,350 : INFO : EPOCH 3 - PROGRESS: at 61.79% examples, 99343 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:10,502 : INFO : EPOCH 3 - PROGRESS: at 63.73% examples, 99270 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:11,592 : INFO : EPOCH 3 - PROGRESS: at 65.63% examples, 99375 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:12,677 : INFO : EPOCH 3 - PROGRESS: at 67.63% examples, 99492 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:13,793 : INFO : EPOCH 3 - PROGRESS: at 69.59% examples, 99526 words/s, in_qsize 7, out_qsize 2\n",
            "2021-04-08 04:41:14,888 : INFO : EPOCH 3 - PROGRESS: at 71.58% examples, 99587 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:15,901 : INFO : EPOCH 3 - PROGRESS: at 73.37% examples, 99702 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:16,925 : INFO : EPOCH 3 - PROGRESS: at 75.24% examples, 99738 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:17,969 : INFO : EPOCH 3 - PROGRESS: at 77.14% examples, 99751 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:19,100 : INFO : EPOCH 3 - PROGRESS: at 78.95% examples, 99590 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:20,225 : INFO : EPOCH 3 - PROGRESS: at 80.90% examples, 99579 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:21,236 : INFO : EPOCH 3 - PROGRESS: at 82.81% examples, 99668 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:22,257 : INFO : EPOCH 3 - PROGRESS: at 84.57% examples, 99598 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:23,295 : INFO : EPOCH 3 - PROGRESS: at 86.37% examples, 99645 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:24,335 : INFO : EPOCH 3 - PROGRESS: at 88.02% examples, 99537 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:41:25,437 : INFO : EPOCH 3 - PROGRESS: at 89.99% examples, 99593 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:26,565 : INFO : EPOCH 3 - PROGRESS: at 91.83% examples, 99578 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:27,577 : INFO : EPOCH 3 - PROGRESS: at 93.69% examples, 99652 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:28,682 : INFO : EPOCH 3 - PROGRESS: at 95.75% examples, 99695 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:29,711 : INFO : EPOCH 3 - PROGRESS: at 97.65% examples, 99733 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:30,735 : INFO : EPOCH 3 - PROGRESS: at 99.24% examples, 99650 words/s, in_qsize 6, out_qsize 0\n",
            "2021-04-08 04:41:30,888 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-08 04:41:30,969 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 04:41:30,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 04:41:31,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 04:41:31,021 : INFO : EPOCH - 3 : training on 8036512 raw words (5720563 effective words) took 57.3s, 99862 effective words/s\n",
            "2021-04-08 04:41:32,200 : INFO : EPOCH 4 - PROGRESS: at 1.66% examples, 84113 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:33,253 : INFO : EPOCH 4 - PROGRESS: at 3.79% examples, 94857 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:34,332 : INFO : EPOCH 4 - PROGRESS: at 5.75% examples, 97633 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:41:35,430 : INFO : EPOCH 4 - PROGRESS: at 7.69% examples, 98710 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:36,509 : INFO : EPOCH 4 - PROGRESS: at 9.58% examples, 99569 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:37,519 : INFO : EPOCH 4 - PROGRESS: at 11.57% examples, 101355 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:38,604 : INFO : EPOCH 4 - PROGRESS: at 13.27% examples, 99752 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:39,819 : INFO : EPOCH 4 - PROGRESS: at 15.24% examples, 99464 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-08 04:41:40,935 : INFO : EPOCH 4 - PROGRESS: at 17.00% examples, 99662 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:41,932 : INFO : EPOCH 4 - PROGRESS: at 19.07% examples, 100709 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:42,978 : INFO : EPOCH 4 - PROGRESS: at 20.65% examples, 99526 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-08 04:41:44,044 : INFO : EPOCH 4 - PROGRESS: at 22.64% examples, 99974 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:45,194 : INFO : EPOCH 4 - PROGRESS: at 24.62% examples, 99792 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:46,336 : INFO : EPOCH 4 - PROGRESS: at 26.48% examples, 99564 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:47,453 : INFO : EPOCH 4 - PROGRESS: at 28.60% examples, 99611 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:48,595 : INFO : EPOCH 4 - PROGRESS: at 30.41% examples, 99473 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:49,632 : INFO : EPOCH 4 - PROGRESS: at 32.04% examples, 99117 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:50,636 : INFO : EPOCH 4 - PROGRESS: at 33.89% examples, 99393 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:51,721 : INFO : EPOCH 4 - PROGRESS: at 35.71% examples, 98913 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:52,757 : INFO : EPOCH 4 - PROGRESS: at 37.52% examples, 99038 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:53,768 : INFO : EPOCH 4 - PROGRESS: at 39.18% examples, 98943 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:41:54,784 : INFO : EPOCH 4 - PROGRESS: at 40.90% examples, 98813 words/s, in_qsize 7, out_qsize 1\n",
            "2021-04-08 04:41:55,895 : INFO : EPOCH 4 - PROGRESS: at 42.93% examples, 98911 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:41:56,915 : INFO : EPOCH 4 - PROGRESS: at 44.88% examples, 99324 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:57,945 : INFO : EPOCH 4 - PROGRESS: at 46.69% examples, 99447 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:41:59,079 : INFO : EPOCH 4 - PROGRESS: at 48.68% examples, 99426 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:00,192 : INFO : EPOCH 4 - PROGRESS: at 50.71% examples, 99478 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:42:01,197 : INFO : EPOCH 4 - PROGRESS: at 52.32% examples, 99203 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:02,327 : INFO : EPOCH 4 - PROGRESS: at 54.30% examples, 99198 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:03,351 : INFO : EPOCH 4 - PROGRESS: at 56.26% examples, 99533 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:04,399 : INFO : EPOCH 4 - PROGRESS: at 57.91% examples, 99326 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:05,442 : INFO : EPOCH 4 - PROGRESS: at 59.68% examples, 99374 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:06,442 : INFO : EPOCH 4 - PROGRESS: at 61.55% examples, 99525 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:07,492 : INFO : EPOCH 4 - PROGRESS: at 63.39% examples, 99545 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:08,614 : INFO : EPOCH 4 - PROGRESS: at 65.16% examples, 99358 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:09,651 : INFO : EPOCH 4 - PROGRESS: at 66.89% examples, 99229 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:42:10,767 : INFO : EPOCH 4 - PROGRESS: at 68.90% examples, 99279 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:11,875 : INFO : EPOCH 4 - PROGRESS: at 70.82% examples, 99327 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:42:12,976 : INFO : EPOCH 4 - PROGRESS: at 72.79% examples, 99391 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:14,021 : INFO : EPOCH 4 - PROGRESS: at 74.71% examples, 99568 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:15,064 : INFO : EPOCH 4 - PROGRESS: at 76.75% examples, 99737 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:16,150 : INFO : EPOCH 4 - PROGRESS: at 78.60% examples, 99667 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:17,178 : INFO : EPOCH 4 - PROGRESS: at 80.37% examples, 99726 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:18,307 : INFO : EPOCH 4 - PROGRESS: at 82.38% examples, 99714 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:19,428 : INFO : EPOCH 4 - PROGRESS: at 84.44% examples, 99711 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:20,540 : INFO : EPOCH 4 - PROGRESS: at 86.34% examples, 99750 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:21,557 : INFO : EPOCH 4 - PROGRESS: at 88.14% examples, 99828 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:22,604 : INFO : EPOCH 4 - PROGRESS: at 89.86% examples, 99718 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:23,728 : INFO : EPOCH 4 - PROGRESS: at 91.75% examples, 99695 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:24,826 : INFO : EPOCH 4 - PROGRESS: at 93.83% examples, 99864 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:25,923 : INFO : EPOCH 4 - PROGRESS: at 95.87% examples, 99918 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:27,007 : INFO : EPOCH 4 - PROGRESS: at 97.89% examples, 99971 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:27,983 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-08 04:42:28,072 : INFO : EPOCH 4 - PROGRESS: at 99.78% examples, 100063 words/s, in_qsize 2, out_qsize 1\n",
            "2021-04-08 04:42:28,074 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 04:42:28,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 04:42:28,111 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 04:42:28,112 : INFO : EPOCH - 4 : training on 8036512 raw words (5720465 effective words) took 57.1s, 100212 effective words/s\n",
            "2021-04-08 04:42:29,198 : INFO : EPOCH 5 - PROGRESS: at 1.55% examples, 85049 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:30,285 : INFO : EPOCH 5 - PROGRESS: at 3.63% examples, 94159 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:31,342 : INFO : EPOCH 5 - PROGRESS: at 5.62% examples, 97871 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:42:32,364 : INFO : EPOCH 5 - PROGRESS: at 7.45% examples, 99002 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:33,448 : INFO : EPOCH 5 - PROGRESS: at 9.20% examples, 98413 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:34,561 : INFO : EPOCH 5 - PROGRESS: at 11.19% examples, 98788 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:42:35,571 : INFO : EPOCH 5 - PROGRESS: at 13.14% examples, 100379 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:36,602 : INFO : EPOCH 5 - PROGRESS: at 14.79% examples, 99723 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-08 04:42:37,650 : INFO : EPOCH 5 - PROGRESS: at 16.59% examples, 100461 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:38,678 : INFO : EPOCH 5 - PROGRESS: at 18.46% examples, 100619 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:39,739 : INFO : EPOCH 5 - PROGRESS: at 20.17% examples, 99887 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:40,757 : INFO : EPOCH 5 - PROGRESS: at 22.18% examples, 100726 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:41,788 : INFO : EPOCH 5 - PROGRESS: at 24.04% examples, 100775 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:42,844 : INFO : EPOCH 5 - PROGRESS: at 25.78% examples, 100613 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:43,967 : INFO : EPOCH 5 - PROGRESS: at 27.81% examples, 100559 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:45,077 : INFO : EPOCH 5 - PROGRESS: at 29.71% examples, 100516 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:46,101 : INFO : EPOCH 5 - PROGRESS: at 31.48% examples, 100601 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:47,223 : INFO : EPOCH 5 - PROGRESS: at 33.40% examples, 100514 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:48,224 : INFO : EPOCH 5 - PROGRESS: at 35.18% examples, 100344 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:49,296 : INFO : EPOCH 5 - PROGRESS: at 37.05% examples, 100248 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:50,356 : INFO : EPOCH 5 - PROGRESS: at 38.85% examples, 100158 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:51,435 : INFO : EPOCH 5 - PROGRESS: at 40.77% examples, 100311 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:52,506 : INFO : EPOCH 5 - PROGRESS: at 42.80% examples, 100503 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:53,664 : INFO : EPOCH 5 - PROGRESS: at 44.62% examples, 100047 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:54,705 : INFO : EPOCH 5 - PROGRESS: at 46.57% examples, 100364 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:55,724 : INFO : EPOCH 5 - PROGRESS: at 48.44% examples, 100473 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:56,732 : INFO : EPOCH 5 - PROGRESS: at 50.32% examples, 100606 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:57,772 : INFO : EPOCH 5 - PROGRESS: at 52.19% examples, 100644 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:58,857 : INFO : EPOCH 5 - PROGRESS: at 54.16% examples, 100738 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:42:59,864 : INFO : EPOCH 5 - PROGRESS: at 56.02% examples, 100846 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:00,867 : INFO : EPOCH 5 - PROGRESS: at 57.78% examples, 100963 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:01,917 : INFO : EPOCH 5 - PROGRESS: at 59.45% examples, 100726 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:02,930 : INFO : EPOCH 5 - PROGRESS: at 61.14% examples, 100605 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:04,079 : INFO : EPOCH 5 - PROGRESS: at 63.15% examples, 100505 words/s, in_qsize 6, out_qsize 1\n",
            "2021-04-08 04:43:05,186 : INFO : EPOCH 5 - PROGRESS: at 65.05% examples, 100522 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:06,251 : INFO : EPOCH 5 - PROGRESS: at 66.98% examples, 100661 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:07,333 : INFO : EPOCH 5 - PROGRESS: at 69.01% examples, 100742 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:08,337 : INFO : EPOCH 5 - PROGRESS: at 70.82% examples, 100848 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:09,484 : INFO : EPOCH 5 - PROGRESS: at 72.79% examples, 100769 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:10,588 : INFO : EPOCH 5 - PROGRESS: at 74.71% examples, 100774 words/s, in_qsize 8, out_qsize 1\n",
            "2021-04-08 04:43:11,682 : INFO : EPOCH 5 - PROGRESS: at 76.77% examples, 100797 words/s, in_qsize 5, out_qsize 2\n",
            "2021-04-08 04:43:12,723 : INFO : EPOCH 5 - PROGRESS: at 78.73% examples, 100967 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:13,763 : INFO : EPOCH 5 - PROGRESS: at 80.53% examples, 100968 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:14,806 : INFO : EPOCH 5 - PROGRESS: at 82.38% examples, 100959 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:15,917 : INFO : EPOCH 5 - PROGRESS: at 84.44% examples, 100963 words/s, in_qsize 8, out_qsize 0\n",
            "2021-04-08 04:43:17,028 : INFO : EPOCH 5 - PROGRESS: at 86.34% examples, 100976 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:18,043 : INFO : EPOCH 5 - PROGRESS: at 88.26% examples, 101176 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:19,101 : INFO : EPOCH 5 - PROGRESS: at 89.99% examples, 101007 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:20,193 : INFO : EPOCH 5 - PROGRESS: at 91.84% examples, 101032 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:21,240 : INFO : EPOCH 5 - PROGRESS: at 93.83% examples, 101143 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:22,426 : INFO : EPOCH 5 - PROGRESS: at 95.87% examples, 101002 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:23,560 : INFO : EPOCH 5 - PROGRESS: at 98.00% examples, 101072 words/s, in_qsize 7, out_qsize 0\n",
            "2021-04-08 04:43:24,517 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-04-08 04:43:24,537 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-04-08 04:43:24,596 : INFO : EPOCH 5 - PROGRESS: at 99.88% examples, 101175 words/s, in_qsize 1, out_qsize 1\n",
            "2021-04-08 04:43:24,598 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-04-08 04:43:24,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-04-08 04:43:24,612 : INFO : EPOCH - 5 : training on 8036512 raw words (5721109 effective words) took 56.5s, 101270 effective words/s\n",
            "2021-04-08 04:43:24,618 : INFO : training on a 40182560 raw words (28603015 effective words) took 288.2s, 99260 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 29s, sys: 1.39 s, total: 9min 30s\n",
            "Wall time: 4min 54s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUZHFsj8RaGo"
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWfcUVixRaGr"
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GMe1dCfRaGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b24b3a-ff54-4da6-8aec-dddceb5abdaf"
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (35000, 300)  Test features shape: (15000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mSFQ9H0RaGy"
      },
      "source": [
        "## Modeling with deep neural networks "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oz9gtU9RaGz"
      },
      "source": [
        "### Building Deep neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCI0uEBIRaGz"
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cQpTH6FRaG1"
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBzbo-YlRaG4"
      },
      "source": [
        "### Visualize sample deep architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fuhzYagRaG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d4b5cf-02d0-4294-c276-4bfbbabd53cd"
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               154112    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 351,489\n",
            "Trainable params: 351,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-COslmPRaG8"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kcisAskRaG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8c0ad7-2be4-4c9f-f517-9ec3e33e8eac"
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "315/315 [==============================] - 4s 3ms/step - loss: 0.3700 - accuracy: 0.8364 - val_loss: 0.3023 - val_accuracy: 0.8786\n",
            "Epoch 2/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2895 - accuracy: 0.8794 - val_loss: 0.3027 - val_accuracy: 0.8780\n",
            "Epoch 3/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2887 - accuracy: 0.8838 - val_loss: 0.3014 - val_accuracy: 0.8811\n",
            "Epoch 4/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2731 - accuracy: 0.8896 - val_loss: 0.3072 - val_accuracy: 0.8749\n",
            "Epoch 5/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2695 - accuracy: 0.8892 - val_loss: 0.2947 - val_accuracy: 0.8754\n",
            "Epoch 6/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2567 - accuracy: 0.8941 - val_loss: 0.3029 - val_accuracy: 0.8734\n",
            "Epoch 7/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2543 - accuracy: 0.8964 - val_loss: 0.3025 - val_accuracy: 0.8783\n",
            "Epoch 8/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2403 - accuracy: 0.9026 - val_loss: 0.3115 - val_accuracy: 0.8754\n",
            "Epoch 9/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2338 - accuracy: 0.9036 - val_loss: 0.3016 - val_accuracy: 0.8794\n",
            "Epoch 10/10\n",
            "315/315 [==============================] - 1s 2ms/step - loss: 0.2265 - accuracy: 0.9045 - val_loss: 0.3382 - val_accuracy: 0.8754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fac1e8e0790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C1gcc4iRaG-"
      },
      "source": [
        "y_pred = (w2v_dnn.predict(avg_wv_test_features)>0.5).astype('int32').ravel()\n",
        "predictions = le.inverse_transform(y_pred) "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XHEZ-X9RaG_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "52c6d2e9-bc92-4c85-a80a-0e79d44f166f"
      },
      "source": [
        "labels = ['negative', 'positive']\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions), index=labels, columns=labels)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.87      0.88      7490\n",
            "    positive       0.87      0.89      0.88      7510\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>6515</td>\n",
              "      <td>975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>839</td>\n",
              "      <td>6671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          negative  positive\n",
              "negative      6515       975\n",
              "positive       839      6671"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZa526UczwTX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}